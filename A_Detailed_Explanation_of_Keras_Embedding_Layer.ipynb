{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Detailed Explanation of Keras Embedding Layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2JiP6cwdzuvvXgHrFszxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koojayeong/TextAnalysis/blob/main/A_Detailed_Explanation_of_Keras_Embedding_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "wQinaInOfgOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykSRWkmYeo_v",
        "outputId": "1f195d6d-535b-49b7-e897-6bc7d6add60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install kaggle \n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!kaggle competitions list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hbmPFAWeyPS",
        "outputId": "001d430e-f065-45f0-ae2c-266f11abc2d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=4273bbef0c50f4710c92a2a02ffaaecb6d8bac0f784db8041da077862e675d06\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/kaggle/BagsOfPopcorn/labeledTrainData.tsv.zip\", \n",
        "                    header=0, # the first line of the file contains column names\n",
        "                    delimiter=\"\\t\", # the fields are separated by tabs\n",
        "                    quoting=3) # ignore doubled quotes"
      ],
      "metadata": {
        "id": "LWwyiY7de0Rc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Embedding Layer"
      ],
      "metadata": {
        "id": "qW3jvWpyfjwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras의 임베딩 레이어는 더 높은 차원의 데이터를 더 낮은 차원의 벡터 공간에 임베딩하기 위해 임베딩을 만들고 싶을 때 사용할 수 있다."
      ],
      "metadata": {
        "id": "FW7ki4LgfvQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Modules"
      ],
      "metadata": {
        "id": "eloZMV0agnjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "import warnings \n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 시각화, 조작\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "\n",
        "# 설정\n",
        "%matplotlib inline\n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid', color_codes=True)\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "\n",
        "# stop-words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# tokenizing\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#keras \n",
        "import keras\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Input\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8QGXEPQff2z",
        "outputId": "b2587f31-a0a7-411e-be1d-91c66230f8a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Sample Corpus of Documents(texts)"
      ],
      "metadata": {
        "id": "QQBdXnEtgtZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_1=\"bitty bought a bit of butter\"\n",
        "sample_text_2=\"but the bit of butter was a bit bitter\"\n",
        "sample_text_3=\"so she bought some better butter to make the bitter butter better\"\n",
        "\n",
        "corp = [sample_text_1, sample_text_2, sample_text_3]\n",
        "no_docs=len(corp)"
      ],
      "metadata": {
        "id": "CkSZl5Eegysu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integer Encoding "
      ],
      "metadata": {
        "id": "-yfoU-uogz0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수 인코딩  \n",
        "각 단어들을 정수로 표현.  \n",
        "예를 들어 \"버터\"는 모든 문서에서 31로 표시된다.  \n",
        "  \n",
        "keras의 one_hot 함수를 사용할 것이다. vocab_size는 각 정수 인코딩을 확실히 할 만큼 충분히 커야 한다. "
      ],
      "metadata": {
        "id": "Eiw9F3kPlLNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=50\n",
        "encod_corp=[]\n",
        "for i,doc in enumerate(corp):\n",
        "  encod_corp.append(one_hot(doc, 50))\n",
        "  print(\"The encoding for document\",i+1,\"is : \",\n",
        "        one_hot(doc,50))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELE88lp2g6tj",
        "outputId": "61f65da8-0ee2-4a69-fc90-7276a6965a99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The encoding for document 1 is :  [49, 17, 19, 16, 4, 9]\n",
            "The encoding for document 2 is :  [45, 23, 16, 4, 9, 39, 19, 16, 1]\n",
            "The encoding for document 3 is :  [30, 14, 17, 32, 6, 9, 48, 48, 23, 1, 9, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding the docs"
      ],
      "metadata": {
        "id": "iGZx1sSCg6-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 문서의 길이를 동일하게 맞춰주기  \n",
        "  \n",
        "keras embedding layer에서는 모든 문서의 길이가 같아야 한다.   \n",
        "pad_sequences 함수를 사용"
      ],
      "metadata": {
        "id": "Dy2glJ4xmLsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "maxlen= -1\n",
        "for doc in corp:\n",
        "  tokens = nltk.word_tokenize(doc)\n",
        "  if maxlen<len(tokens):\n",
        "    maxlen=len(tokens)\n",
        "\n",
        "print(\"The maximum number of words in any document is : \",\n",
        "      maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv9mYQUeg9Q1",
        "outputId": "92acddb2-bb62-4662-a3e6-a67c8e5594f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "The maximum number of words in any document is :  12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding\n",
        "pad_corp = pad_sequences(encod_corp,\n",
        "                         maxlen=maxlen,\n",
        "                         padding='post',\n",
        "                         value=0.0)\n",
        "print(\"No of padded documents: \", len(pad_corp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbFBtJ57g9eb",
        "outputId": "9b8b1ee1-802d-449f-9447-bc6ab24ae70f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of padded documents:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(pad_corp):\n",
        "  print(\"The padded encoding for document\", i+1,\n",
        "        \"is : \", doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dETq0s0QpXSH",
        "outputId": "953c5bbf-c983-46e5-d5a0-ea27ae4ab207"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The padded encoding for document 1 is :  [49 17 19 16  4  9  0  0  0  0  0  0]\n",
            "The padded encoding for document 2 is :  [45 23 16  4  9 39 19 16  1  0  0  0]\n",
            "The padded encoding for document 3 is :  [30 14 17 32  6  9 48 48 23  1  9  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Embedding using Keras Embedding Layer"
      ],
      "metadata": {
        "id": "G7ve4aPbg9wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specifying the input shape\n",
        "input = Input(shape=(no_docs,maxlen), dtype='float64')"
      ],
      "metadata": {
        "id": "L6FlpyrGpn62"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "shape of input.\n",
        "각 문서는 12개의 요소를 갖는다\n",
        "'''\n",
        "word_input = Input(shape=(maxlen,),dtype='float64')\n",
        "\n",
        "# creating the embedding\n",
        "word_embedding = Embedding(input_dim=vocab_size,\n",
        "                           output_dim=8,\n",
        "                           input_length=maxlen)(word_input)\n",
        "\n",
        "word_vec = Flatten()(word_embedding) # flatten\n",
        "embed_model = Model([word_input], word_vec)\n"
      ],
      "metadata": {
        "id": "gu_Uic-apy32"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**parameters**\n",
        "  \n",
        "* 'input_dim'   \n",
        "the vocab size that we will choose. \n",
        "\n",
        "* 'output_dim'   \n",
        "the number of dimensions we wish to embed into.\n",
        "\n",
        "* 'input_length'   \n",
        "lenght of the maximum document."
      ],
      "metadata": {
        "id": "6KSA9d1bhFrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "embed_model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['acc'])"
      ],
      "metadata": {
        "id": "oQvmnK-OhC-x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(word_embedding))\n",
        "print(word_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6LfFm_fr1KZ",
        "outputId": "74e0f848-3f47-4eb8-cda0-1824455f5d59"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'keras.engine.keras_tensor.KerasTensor'>\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 12, 8), dtype=tf.float32, name=None), name='embedding/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embed_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNMjEpBTsI6j",
        "outputId": "ccd5e175-0eb0-42e9-a6f7-281b834a11ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 12)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 12, 8)             400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 96)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 400\n",
            "Trainable params: 400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed_model.predict(pad_corp)"
      ],
      "metadata": {
        "id": "OA4_OglStLkG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of embeddings : \", embeddings.shape)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia8IpoGKtPKY",
        "outputId": "857b2f5f-7580-4ea7-ced0-3c41d673e7f7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings :  (3, 96)\n",
            "[[ 0.00736635 -0.00802261 -0.00775436 -0.03623832 -0.03932606 -0.0015465\n",
            "   0.01801804 -0.03678013 -0.00314764  0.025275   -0.04289455  0.04709974\n",
            "  -0.0257776   0.04964327 -0.02188355 -0.00025401  0.00933676 -0.04635463\n",
            "  -0.01522933 -0.04043658  0.04828198 -0.01840921 -0.03717828  0.04608713\n",
            "  -0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209 -0.0239241\n",
            "  -0.01161025 -0.02549825 -0.01337262 -0.01095115 -0.01784123  0.02232993\n",
            "  -0.01370894 -0.02049881  0.0063342  -0.04695794 -0.01352566 -0.03218435\n",
            "   0.01499856  0.02114407  0.0248417  -0.02575964  0.02533844 -0.0162251\n",
            "   0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "   0.04266334  0.0208007   0.04456404 -0.01751643  0.03412116 -0.01371187\n",
            "  -0.03960695 -0.01843982  0.04266334  0.0208007   0.04456404 -0.01751643\n",
            "   0.03412116 -0.01371187 -0.03960695 -0.01843982  0.04266334  0.0208007\n",
            "   0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "   0.04266334  0.0208007   0.04456404 -0.01751643  0.03412116 -0.01371187\n",
            "  -0.03960695 -0.01843982  0.04266334  0.0208007   0.04456404 -0.01751643\n",
            "   0.03412116 -0.01371187 -0.03960695 -0.01843982  0.04266334  0.0208007 ]\n",
            " [-0.01390813 -0.01311709  0.00856952  0.01425666 -0.02233439  0.03981135\n",
            "   0.01633121  0.00797454 -0.0205754  -0.01290308 -0.01870932 -0.03936666\n",
            "   0.03962804 -0.02333468 -0.02539053 -0.02832767 -0.03437523  0.01135999\n",
            "  -0.00389359 -0.0236358   0.03599209 -0.0239241  -0.01161025 -0.02549825\n",
            "  -0.01337262 -0.01095115 -0.01784123  0.02232993 -0.01370894 -0.02049881\n",
            "   0.0063342  -0.04695794 -0.01352566 -0.03218435  0.01499856  0.02114407\n",
            "   0.0248417  -0.02575964  0.02533844 -0.0162251  -0.03095253  0.00633843\n",
            "  -0.04040259  0.02227003  0.00960692  0.02267772 -0.03722551  0.00038435\n",
            "   0.00933676 -0.04635463 -0.01522933 -0.04043658  0.04828198 -0.01840921\n",
            "  -0.03717828  0.04608713 -0.03437523  0.01135999 -0.00389359 -0.0236358\n",
            "   0.03599209 -0.0239241  -0.01161025 -0.02549825  0.01747729  0.00014557\n",
            "  -0.04775028  0.0327588  -0.02742192  0.02844062  0.03467659  0.02700533\n",
            "   0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "   0.04266334  0.0208007   0.04456404 -0.01751643  0.03412116 -0.01371187\n",
            "  -0.03960695 -0.01843982  0.04266334  0.0208007   0.04456404 -0.01751643\n",
            "   0.03412116 -0.01371187 -0.03960695 -0.01843982  0.04266334  0.0208007 ]\n",
            " [ 0.01329723 -0.04102396  0.01195316  0.00741936 -0.01405082  0.03006916\n",
            "  -0.01114162 -0.04193677 -0.01624135 -0.03084036  0.03932    -0.00643486\n",
            "   0.03613261 -0.01706742  0.01479817  0.04377139 -0.00314764  0.025275\n",
            "  -0.04289455  0.04709974 -0.0257776   0.04964327 -0.02188355 -0.00025401\n",
            "  -0.02747698 -0.01456089 -0.04909508  0.04107158  0.04010998  0.01296014\n",
            "   0.04290814 -0.04617956 -0.04714689  0.03914061  0.00801038 -0.04068355\n",
            "  -0.00294503 -0.035999    0.02034792  0.04354734 -0.01352566 -0.03218435\n",
            "   0.01499856  0.02114407  0.0248417  -0.02575964  0.02533844 -0.0162251\n",
            "   0.00748504  0.03882251  0.03295111  0.01089406  0.03285077 -0.00650343\n",
            "  -0.0064187   0.04562763  0.00748504  0.03882251  0.03295111  0.01089406\n",
            "   0.03285077 -0.00650343 -0.0064187   0.04562763 -0.0205754  -0.01290308\n",
            "  -0.01870932 -0.03936666  0.03962804 -0.02333468 -0.02539053 -0.02832767\n",
            "   0.01747729  0.00014557 -0.04775028  0.0327588  -0.02742192  0.02844062\n",
            "   0.03467659  0.02700533 -0.01352566 -0.03218435  0.01499856  0.02114407\n",
            "   0.0248417  -0.02575964  0.02533844 -0.0162251  -0.04714689  0.03914061\n",
            "   0.00801038 -0.04068355 -0.00294503 -0.035999    0.02034792  0.04354734]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embeddings.reshape(-1, maxlen, 8)\n",
        "print(\"Shape of embeddings : \", embeddings.shape)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4fY33jat8Oo",
        "outputId": "fbc7d411-0ea3-4b7b-87aa-e06791fce542"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings :  (3, 12, 8)\n",
            "[[[ 0.00736635 -0.00802261 -0.00775436 -0.03623832 -0.03932606\n",
            "   -0.0015465   0.01801804 -0.03678013]\n",
            "  [-0.00314764  0.025275   -0.04289455  0.04709974 -0.0257776\n",
            "    0.04964327 -0.02188355 -0.00025401]\n",
            "  [ 0.00933676 -0.04635463 -0.01522933 -0.04043658  0.04828198\n",
            "   -0.01840921 -0.03717828  0.04608713]\n",
            "  [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209\n",
            "   -0.0239241  -0.01161025 -0.02549825]\n",
            "  [-0.01337262 -0.01095115 -0.01784123  0.02232993 -0.01370894\n",
            "   -0.02049881  0.0063342  -0.04695794]\n",
            "  [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417\n",
            "   -0.02575964  0.02533844 -0.0162251 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]]\n",
            "\n",
            " [[-0.01390813 -0.01311709  0.00856952  0.01425666 -0.02233439\n",
            "    0.03981135  0.01633121  0.00797454]\n",
            "  [-0.0205754  -0.01290308 -0.01870932 -0.03936666  0.03962804\n",
            "   -0.02333468 -0.02539053 -0.02832767]\n",
            "  [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209\n",
            "   -0.0239241  -0.01161025 -0.02549825]\n",
            "  [-0.01337262 -0.01095115 -0.01784123  0.02232993 -0.01370894\n",
            "   -0.02049881  0.0063342  -0.04695794]\n",
            "  [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417\n",
            "   -0.02575964  0.02533844 -0.0162251 ]\n",
            "  [-0.03095253  0.00633843 -0.04040259  0.02227003  0.00960692\n",
            "    0.02267772 -0.03722551  0.00038435]\n",
            "  [ 0.00933676 -0.04635463 -0.01522933 -0.04043658  0.04828198\n",
            "   -0.01840921 -0.03717828  0.04608713]\n",
            "  [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209\n",
            "   -0.0239241  -0.01161025 -0.02549825]\n",
            "  [ 0.01747729  0.00014557 -0.04775028  0.0327588  -0.02742192\n",
            "    0.02844062  0.03467659  0.02700533]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]\n",
            "  [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695\n",
            "   -0.01843982  0.04266334  0.0208007 ]]\n",
            "\n",
            " [[ 0.01329723 -0.04102396  0.01195316  0.00741936 -0.01405082\n",
            "    0.03006916 -0.01114162 -0.04193677]\n",
            "  [-0.01624135 -0.03084036  0.03932    -0.00643486  0.03613261\n",
            "   -0.01706742  0.01479817  0.04377139]\n",
            "  [-0.00314764  0.025275   -0.04289455  0.04709974 -0.0257776\n",
            "    0.04964327 -0.02188355 -0.00025401]\n",
            "  [-0.02747698 -0.01456089 -0.04909508  0.04107158  0.04010998\n",
            "    0.01296014  0.04290814 -0.04617956]\n",
            "  [-0.04714689  0.03914061  0.00801038 -0.04068355 -0.00294503\n",
            "   -0.035999    0.02034792  0.04354734]\n",
            "  [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417\n",
            "   -0.02575964  0.02533844 -0.0162251 ]\n",
            "  [ 0.00748504  0.03882251  0.03295111  0.01089406  0.03285077\n",
            "   -0.00650343 -0.0064187   0.04562763]\n",
            "  [ 0.00748504  0.03882251  0.03295111  0.01089406  0.03285077\n",
            "   -0.00650343 -0.0064187   0.04562763]\n",
            "  [-0.0205754  -0.01290308 -0.01870932 -0.03936666  0.03962804\n",
            "   -0.02333468 -0.02539053 -0.02832767]\n",
            "  [ 0.01747729  0.00014557 -0.04775028  0.0327588  -0.02742192\n",
            "    0.02844062  0.03467659  0.02700533]\n",
            "  [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417\n",
            "   -0.02575964  0.02533844 -0.0162251 ]\n",
            "  [-0.04714689  0.03914061  0.00801038 -0.04068355 -0.00294503\n",
            "   -0.035999    0.02034792  0.04354734]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(3,12,8)**\n",
        "  \n",
        "3: 문서 개수  \n",
        "12 : 각 문서가 12개의 단어로 이루어짐(maxlen)  \n",
        "8 : 각 단어가 8차원"
      ],
      "metadata": {
        "id": "dJLAvvq8uZFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Encoding for a particular word in a specific document"
      ],
      "metadata": {
        "id": "cOhXFGp5hDXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(embeddings):\n",
        "  for j,word in enumerate(doc):\n",
        "    print(\"\\n\\nThe encoding for \", str(j+1)+\"th word\",\n",
        "          \"in\", str(i+1)+\"th document is : \\n\", word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kiqic4GuYr4",
        "outputId": "c2f6c9c9-b567-4460-e36b-991652c41945"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The encoding for  1th word in 1th document is : \n",
            " [ 0.00736635 -0.00802261 -0.00775436 -0.03623832 -0.03932606 -0.0015465\n",
            "  0.01801804 -0.03678013]\n",
            "\n",
            "\n",
            "The encoding for  2th word in 1th document is : \n",
            " [-0.00314764  0.025275   -0.04289455  0.04709974 -0.0257776   0.04964327\n",
            " -0.02188355 -0.00025401]\n",
            "\n",
            "\n",
            "The encoding for  3th word in 1th document is : \n",
            " [ 0.00933676 -0.04635463 -0.01522933 -0.04043658  0.04828198 -0.01840921\n",
            " -0.03717828  0.04608713]\n",
            "\n",
            "\n",
            "The encoding for  4th word in 1th document is : \n",
            " [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209 -0.0239241\n",
            " -0.01161025 -0.02549825]\n",
            "\n",
            "\n",
            "The encoding for  5th word in 1th document is : \n",
            " [-0.01337262 -0.01095115 -0.01784123  0.02232993 -0.01370894 -0.02049881\n",
            "  0.0063342  -0.04695794]\n",
            "\n",
            "\n",
            "The encoding for  6th word in 1th document is : \n",
            " [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417  -0.02575964\n",
            "  0.02533844 -0.0162251 ]\n",
            "\n",
            "\n",
            "The encoding for  7th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  8th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  9th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  10th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  11th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  12th word in 1th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  1th word in 2th document is : \n",
            " [-0.01390813 -0.01311709  0.00856952  0.01425666 -0.02233439  0.03981135\n",
            "  0.01633121  0.00797454]\n",
            "\n",
            "\n",
            "The encoding for  2th word in 2th document is : \n",
            " [-0.0205754  -0.01290308 -0.01870932 -0.03936666  0.03962804 -0.02333468\n",
            " -0.02539053 -0.02832767]\n",
            "\n",
            "\n",
            "The encoding for  3th word in 2th document is : \n",
            " [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209 -0.0239241\n",
            " -0.01161025 -0.02549825]\n",
            "\n",
            "\n",
            "The encoding for  4th word in 2th document is : \n",
            " [-0.01337262 -0.01095115 -0.01784123  0.02232993 -0.01370894 -0.02049881\n",
            "  0.0063342  -0.04695794]\n",
            "\n",
            "\n",
            "The encoding for  5th word in 2th document is : \n",
            " [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417  -0.02575964\n",
            "  0.02533844 -0.0162251 ]\n",
            "\n",
            "\n",
            "The encoding for  6th word in 2th document is : \n",
            " [-0.03095253  0.00633843 -0.04040259  0.02227003  0.00960692  0.02267772\n",
            " -0.03722551  0.00038435]\n",
            "\n",
            "\n",
            "The encoding for  7th word in 2th document is : \n",
            " [ 0.00933676 -0.04635463 -0.01522933 -0.04043658  0.04828198 -0.01840921\n",
            " -0.03717828  0.04608713]\n",
            "\n",
            "\n",
            "The encoding for  8th word in 2th document is : \n",
            " [-0.03437523  0.01135999 -0.00389359 -0.0236358   0.03599209 -0.0239241\n",
            " -0.01161025 -0.02549825]\n",
            "\n",
            "\n",
            "The encoding for  9th word in 2th document is : \n",
            " [ 0.01747729  0.00014557 -0.04775028  0.0327588  -0.02742192  0.02844062\n",
            "  0.03467659  0.02700533]\n",
            "\n",
            "\n",
            "The encoding for  10th word in 2th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  11th word in 2th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  12th word in 2th document is : \n",
            " [ 0.04456404 -0.01751643  0.03412116 -0.01371187 -0.03960695 -0.01843982\n",
            "  0.04266334  0.0208007 ]\n",
            "\n",
            "\n",
            "The encoding for  1th word in 3th document is : \n",
            " [ 0.01329723 -0.04102396  0.01195316  0.00741936 -0.01405082  0.03006916\n",
            " -0.01114162 -0.04193677]\n",
            "\n",
            "\n",
            "The encoding for  2th word in 3th document is : \n",
            " [-0.01624135 -0.03084036  0.03932    -0.00643486  0.03613261 -0.01706742\n",
            "  0.01479817  0.04377139]\n",
            "\n",
            "\n",
            "The encoding for  3th word in 3th document is : \n",
            " [-0.00314764  0.025275   -0.04289455  0.04709974 -0.0257776   0.04964327\n",
            " -0.02188355 -0.00025401]\n",
            "\n",
            "\n",
            "The encoding for  4th word in 3th document is : \n",
            " [-0.02747698 -0.01456089 -0.04909508  0.04107158  0.04010998  0.01296014\n",
            "  0.04290814 -0.04617956]\n",
            "\n",
            "\n",
            "The encoding for  5th word in 3th document is : \n",
            " [-0.04714689  0.03914061  0.00801038 -0.04068355 -0.00294503 -0.035999\n",
            "  0.02034792  0.04354734]\n",
            "\n",
            "\n",
            "The encoding for  6th word in 3th document is : \n",
            " [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417  -0.02575964\n",
            "  0.02533844 -0.0162251 ]\n",
            "\n",
            "\n",
            "The encoding for  7th word in 3th document is : \n",
            " [ 0.00748504  0.03882251  0.03295111  0.01089406  0.03285077 -0.00650343\n",
            " -0.0064187   0.04562763]\n",
            "\n",
            "\n",
            "The encoding for  8th word in 3th document is : \n",
            " [ 0.00748504  0.03882251  0.03295111  0.01089406  0.03285077 -0.00650343\n",
            " -0.0064187   0.04562763]\n",
            "\n",
            "\n",
            "The encoding for  9th word in 3th document is : \n",
            " [-0.0205754  -0.01290308 -0.01870932 -0.03936666  0.03962804 -0.02333468\n",
            " -0.02539053 -0.02832767]\n",
            "\n",
            "\n",
            "The encoding for  10th word in 3th document is : \n",
            " [ 0.01747729  0.00014557 -0.04775028  0.0327588  -0.02742192  0.02844062\n",
            "  0.03467659  0.02700533]\n",
            "\n",
            "\n",
            "The encoding for  11th word in 3th document is : \n",
            " [-0.01352566 -0.03218435  0.01499856  0.02114407  0.0248417  -0.02575964\n",
            "  0.02533844 -0.0162251 ]\n",
            "\n",
            "\n",
            "The encoding for  12th word in 3th document is : \n",
            " [-0.04714689  0.03914061  0.00801038 -0.04068355 -0.00294503 -0.035999\n",
            "  0.02034792  0.04354734]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 하면 각각 12개의 단어로 구성되고 각 단어가 8차원 벡터에 매핑된 3개의 문서를 쉽게 시각화할 수 있습니다."
      ],
      "metadata": {
        "id": "iQss1swNwMxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'input_dim' = 선택할 vocab 크기\n",
        "\n",
        "'output_dim' = 포함할 차원 수\n",
        "\n",
        "'input_length' = 최대 문서의 허용치"
      ],
      "metadata": {
        "id": "lsbDEJXhwWmv"
      }
    }
  ]
}